{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
=======
   "execution_count": 2,
>>>>>>> 0fb07151e8844acdf5e772f72a1a19fe86698bf9
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 3,
>>>>>>> 0fb07151e8844acdf5e772f72a1a19fe86698bf9
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard packages\n",
    "import gym\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch as th\n",
    "from torch import nn\n",
    "import os\n",
<<<<<<< HEAD
    "from tqdm import tqdm\n",
=======
>>>>>>> 0fb07151e8844acdf5e772f72a1a19fe86698bf9
    "\n",
    "# mario packages\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros import SuperMarioBrosEnv\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros.actions import *\n",
    "\n",
    "# Import Frame Stacker Wrapper and GrayScaling Wrapper\n",
    "from gym.wrappers import GrayScaleObservation\n",
    "\n",
    "# Import Vectorization Wrappers\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv\n",
    "\n",
    "# Import algo\n",
    "from stable_baselines3 import A2C, PPO\n",
    "\n",
    "# Import Base Callback for saving models\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.vec_env import VecVideoRecorder, SubprocVecEnv, DummyVecEnv"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_logdir = os.path.abspath(\".\")\n",
    "# reward_log_path = os.path.join(tensorboard_logdir, 'reward_log.csv')"
=======
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_logdir = os.path.abspath(\"./mario/cc_model\")\n",
    "reward_log_path = os.path.join(tensorboard_logdir, 'reward_log.csv')"
>>>>>>> 0fb07151e8844acdf5e772f72a1a19fe86698bf9
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 53,
=======
   "execution_count": 5,
>>>>>>> 0fb07151e8844acdf5e772f72a1a19fe86698bf9
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "ERROR: Failed to launch TensorBoard (exited with 1).\n",
       "Contents of stderr:\n",
       "TensorFlow installation not found - running with reduced feature set.\n",
       "Address already in use\n",
       "Port 6006 is in use by another program. Either identify and stop that program, or start the server with a different port."
=======
       "Reusing TensorBoard on port 6006 (pid 15792), started 19:08:40 ago. (Use '!kill 15792' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-bad5fee95a16325f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-bad5fee95a16325f\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
>>>>>>> 0fb07151e8844acdf5e772f72a1a19fe86698bf9
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir $tensorboard_logdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_logdir = os.path.abspath(\"./mario/reg_model\")\n",
    "reward_log_path = os.path.join(tensorboard_logdir, 'reward_log.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6008 (pid 20216), started 19:05:05 ago. (Use '!kill 20216' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-7b68ae9ec771e65f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-7b68ae9ec771e65f\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6008;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir $tensorboard_logdir --port=6008"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check reward"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 8,
>>>>>>> 0fb07151e8844acdf5e772f72a1a19fe86698bf9
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 9,
>>>>>>> 0fb07151e8844acdf5e772f72a1a19fe86698bf9
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "      timesteps     reward  best_reward\n",
      "2328  2328000.0   1857.85        3080.0\n",
      "2120  2120000.0   2141.05        3079.0\n",
      "1749  1749000.0    2303.9        3077.0\n",
      "2370  2370000.0    1960.9        3077.0\n",
      "1418  1418000.0    2113.7        3077.0\n",
      "2489  2489000.0    2031.2        3077.0\n",
      "2255  2255000.0   1991.75        3076.0\n",
      "1375  1375000.0    1837.3        3076.0\n",
      "1934  1934000.0   2331.95        3075.0\n",
      "2147  2147000.0   2091.45        3075.0\n",
      "      timesteps     reward  best_reward\n",
      "2621  2621000.0    2220.5        3069.0\n",
      "2620  2620000.0   1775.05        3061.0\n",
      "2619  2619000.0   2388.55        3062.0\n",
      "2618  2618000.0    1835.3        3067.0\n",
      "2617  2617000.0   1901.75        3057.0\n"
=======
      "     timesteps     reward  best_reward\n",
      "1418  1418000     2113.7        3077.0\n",
      "1375  1375000     1837.3        3076.0\n",
      "1483  1483000     1938.6        3075.0\n",
      "1336  1336000    1881.75        3075.0\n",
      "1486  1486000     1886.9        3074.0\n",
      "1633  1633000     2502.3        3074.0\n",
      "1460  1460000    1585.75        3073.0\n",
      "1389  1389000    1473.05        3073.0\n",
      "1433  1433000    1876.55        3073.0\n",
      "1609  1609000     1632.9        3073.0\n"
>>>>>>> 0fb07151e8844acdf5e772f72a1a19fe86698bf9
     ]
    }
   ],
   "source": [
    "reward_ccmodel = pd.read_csv(\"mario/cc_model/reward_log.csv\")\n",
    "reward_ccmodel = reward_ccmodel.loc[reward_ccmodel.timesteps!=\"timesteps\", :].reset_index(drop=True)\n",
<<<<<<< HEAD
    "reward_ccmodel[\"timesteps\"] = reward_ccmodel[\"timesteps\"].astype(\"float\")\n",
    "reward_ccmodel[\"best_reward\"] = reward_ccmodel[\"best_reward\"].astype(\"float\")\n",
    "reward_ccmodel = reward_ccmodel.sort_values(by=\"best_reward\", ascending=False)\n",
    "print(reward_ccmodel.head(10))\n",
    "print(reward_ccmodel.sort_values(by=\"timesteps\", ascending=False).head(5))\n",
=======
    "reward_ccmodel[\"best_reward\"] = reward_ccmodel[\"best_reward\"].astype(\"float\")\n",
    "reward_ccmodel = reward_ccmodel.sort_values(by=\"best_reward\", ascending=False)\n",
    "print(reward_ccmodel.head(10))\n",
>>>>>>> 0fb07151e8844acdf5e772f72a1a19fe86698bf9
    "# reward_ccmodel.loc[reward_ccmodel.timesteps==\"timesteps\", :]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 15,
=======
   "execution_count": 10,
>>>>>>> 0fb07151e8844acdf5e772f72a1a19fe86698bf9
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      timesteps   reward  best_reward\n",
<<<<<<< HEAD
      "2272    2273000  2289.75       3068.0\n",
      "2447    2448000  2649.50       3068.0\n",
      "2345    2346000  2287.05       3068.0\n",
      "2346    2347000  2334.40       3068.0\n",
      "2351    2352000  2586.05       3068.0\n",
      "2443    2444000  2557.75       3068.0\n",
      "2352    2353000  2543.20       3068.0\n",
      "2425    2426000  2628.05       3068.0\n",
      "2519    2520000  2150.35       3068.0\n",
      "2424    2425000  2517.30       3068.0\n",
      "      timesteps   reward  best_reward\n",
      "2523    2524000  2163.10       3062.0\n",
      "2522    2523000  2032.20       3062.0\n",
      "2521    2522000  2332.50       3063.0\n",
      "2520    2521000  2584.80       3062.0\n",
      "2519    2520000  2150.35       3068.0\n"
=======
      "1449    1450000  2137.15       3067.0\n",
      "1538    1539000  2256.35       3067.0\n",
      "1690    1691000  2343.45       3066.0\n",
      "1550    1551000  1847.85       3065.0\n",
      "1517    1518000  2341.95       3065.0\n",
      "1542    1543000  2461.45       3065.0\n",
      "1531    1532000  1992.00       3065.0\n",
      "1522    1523000  1751.10       3065.0\n",
      "1599    1600000  2164.00       3065.0\n",
      "1072    1073000  1886.85       3065.0\n"
>>>>>>> 0fb07151e8844acdf5e772f72a1a19fe86698bf9
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "reward_regmodel = pd.read_csv(\"mario/reg_model/reward_log.csv\").sort_values(by=\"best_reward\", ascending=False)\n",
    "print(reward_regmodel.head(10))\n",
    "print(reward_regmodel.sort_values(by=\"timesteps\", ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     timesteps  reward  best_reward\n",
      "329     330000  779.45       2971.0\n",
      "70       71000  629.80       2661.0\n",
      "351     352000  827.55       2445.0\n",
      "346     347000  710.35       2293.0\n",
      "322     323000  585.45       2288.0\n",
      "312     313000  751.90       2264.0\n",
      "324     325000  619.85       1861.0\n",
      "119     120000  643.50       1827.0\n",
      "338     339000  824.90       1820.0\n",
      "344     345000  794.70       1814.0\n",
      "     timesteps  reward  best_reward\n",
      "388     389000  596.30       1314.0\n",
      "387     388000  492.15        938.0\n",
      "386     387000  577.30       1053.0\n",
      "385     386000  516.30        966.0\n",
      "384     385000  575.40       1263.0\n"
     ]
    }
   ],
   "source": [
    "reward_ccmodel_a2c = pd.read_csv(\"../actorcritic/mario/cc_model/reward_log.csv\").sort_values(by=\"best_reward\", ascending=False)\n",
    "print(reward_ccmodel_a2c.head(10))\n",
    "print(reward_ccmodel_a2c.sort_values(by=\"timesteps\", ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     timesteps  reward  best_reward\n",
      "218     219000  751.95       2245.0\n",
      "191     192000  684.15       2030.0\n",
      "229     230000  660.80       1864.0\n",
      "217     218000  639.50       1803.0\n",
      "207     208000  660.15       1615.0\n",
      "215     216000  650.70       1607.0\n",
      "278     279000  717.05       1599.0\n",
      "39       40000  558.15       1592.0\n",
      "216     217000  658.45       1565.0\n",
      "205     206000  615.25       1492.0\n",
      "    timesteps  reward  best_reward\n",
      "69     357000  588.75       1195.0\n",
      "68     356000  578.35       1035.0\n",
      "67     355000  430.50        915.0\n",
      "66     354000  482.20        733.0\n",
      "65     353000  584.70       1212.0\n"
     ]
    }
   ],
   "source": [
    "reward_regmodel_a2c_df1 = pd.read_csv(\"../actorcritic/mario/reg_model/reward.csv\").sort_values(by=\"best_reward\", ascending=False)\n",
    "reward_regmodel_a2c_df2 = pd.read_csv(\"../actorcritic/mario/reg_model/reward_log.csv\")\n",
    "reward_regmodel_a2c_df2.columns = reward_regmodel_a2c_df1.columns\n",
    "reward_regmodel_a2c_df2 = reward_regmodel_a2c_df2.sort_values(by=\"best_reward\", ascending=False)\n",
    "reward_regmodel_a2c = pd.concat([reward_regmodel_a2c_df1, reward_regmodel_a2c_df2], axis=0)\n",
    "print(reward_regmodel_a2c.head(10))\n",
    "print(reward_regmodel_a2c.sort_values(by=\"timesteps\", ascending=False).head(5))"
=======
    "reward_ccmodel = pd.read_csv(\"mario/reg_model/reward_log.csv\").sort_values(by=\"best_reward\", ascending=False)\n",
    "print(reward_ccmodel.head(10))"
>>>>>>> 0fb07151e8844acdf5e772f72a1a19fe86698bf9
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check results"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 37,
=======
   "execution_count": 12,
>>>>>>> 0fb07151e8844acdf5e772f72a1a19fe86698bf9
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard packages\n",
    "import gym\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch as th\n",
    "from torch import nn\n",
    "import os\n",
    "\n",
    "# mario packages\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros import SuperMarioBrosEnv\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros.actions import *\n",
    "from gym.wrappers import RecordVideo\n",
    "\n",
    "# Import Frame Stacker Wrapper and GrayScaling Wrapper\n",
    "from gym.wrappers import GrayScaleObservation\n",
    "\n",
    "# Import Vectorization Wrappers\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv\n",
    "\n",
    "# Import algo\n",
    "from stable_baselines3 import A2C, PPO\n",
    "\n",
    "# Import Base Callback for saving models\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.vec_env import VecVideoRecorder, SubprocVecEnv, DummyVecEnv\n",
    "\n",
    "class CoinCollectorSuperMarioBrosEnv(SuperMarioBrosEnv):\n",
    "    #score btn 2 time frames can maybe go upto 8000 so we can just divide by 100 (reference https://www.mariowiki.com/Point)\n",
    "    reward_range = (-15, 100)\n",
    "\n",
    "    def __init__(self, rom_mode='vanilla', lost_levels=False, target=None):\n",
    "        super().__init__(rom_mode=rom_mode, lost_levels=lost_levels, target=target)\n",
    "\n",
    "        # variable to keep track of score deltas\n",
    "        self._score_last = 0\n",
    "\n",
    "    @property\n",
    "    def _score_reward(self):\n",
    "        _reward = self._score - self._score_last\n",
    "        self._score_last = self._score\n",
    "        return _reward/100\n",
    "\n",
    "    # This should override the parent function\n",
    "    def _get_reward(self):\n",
    "        return self._x_reward + self._score_reward + self._time_penalty + self._death_penalty\n",
    "\n",
    "'''\n",
    "The code below registers this new environment in gym for us to reference later. Code borrowed from _registration.py of gym_super_mario_bros\n",
    "'''\n",
    "def _register_coin_collector_mario_stage_env(id, **kwargs):\n",
    "    \"\"\"\n",
    "    Register a Super Mario Bros. (1/2) stage environment with OpenAI Gym.\n",
    "\n",
    "    Args:\n",
    "        id (str): id for the env to register\n",
    "        kwargs (dict): keyword arguments for the SuperMarioBrosEnv initializer\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    \"\"\"\n",
    "    # register the environment\n",
    "    gym.envs.registration.register(\n",
    "        id=id,\n",
    "        # entry_point='.:CoinCollectorSuperMarioBrosEnv',\n",
    "        entry_point=CoinCollectorSuperMarioBrosEnv,\n",
    "        max_episode_steps=9999999,\n",
    "        reward_threshold=9999999,\n",
    "        kwargs=kwargs,\n",
    "        nondeterministic=True,\n",
    "    )\n",
    "\n",
    "def _register_all_coin_collector_envs():\n",
    "    # a template for making individual stage environments\n",
    "    _ID_TEMPLATE = 'CoinCollectorSuperMarioBrosEnv-{}-{}-v{}'\n",
    "    # A list of ROM modes for each level environment\n",
    "    _ROM_MODES = [\n",
    "        'vanilla',\n",
    "        'downsample',\n",
    "        'pixel',\n",
    "        'rectangle'\n",
    "    ]\n",
    "\n",
    "    # iterate over all the rom modes, worlds (1-8), and stages (1-4)\n",
    "    for version, rom_mode in enumerate(_ROM_MODES):\n",
    "        for world in range(1, 9):\n",
    "            for stage in range(1, 5):\n",
    "                # create the target\n",
    "                target = (world, stage)\n",
    "                # setup the frame-skipping environment\n",
    "                env_id = _ID_TEMPLATE.format(world, stage, version)\n",
    "                print(f\"Registering Coin Collector {env_id} in gym for use later on.\")\n",
    "                _register_coin_collector_mario_stage_env(env_id, rom_mode=rom_mode, target=target)\n",
    "                print(f\"Successfully registered coin collector env {env_id}!\")\n",
    "\n",
    "def create_gym_env_from_level(world, stage, version, use_coin_collector_env):\n",
    "    level_suffix = f\"{world}-{stage}-v{version}\"\n",
    "    if not use_coin_collector_env:\n",
    "        level = f\"SuperMarioBros-{level_suffix}\"\n",
    "        env = gym_super_mario_bros.make(level)\n",
    "    else:\n",
    "        env_set = set(gym.envs.registration.registry.env_specs.copy().keys())\n",
    "        level = f\"CoinCollectorSuperMarioBrosEnv-{level_suffix}\"\n",
    "        if level not in env_set:\n",
    "            # register all these custom environments for the first time\n",
    "            _register_all_coin_collector_envs()\n",
    "\n",
    "        assert level in set(\n",
    "            gym.envs.registration.registry.env_specs.copy().keys()\n",
    "        ), f\"Looks like {level} was not registered correctly!\"\n",
    "        env = gym.make(level)\n",
    "\n",
    "    return env\n",
    "\n",
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, info\n",
    "\n",
    "class ResizeEnv(gym.ObservationWrapper):\n",
    "    def __init__(self, env, size):\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        (oldh, oldw, oldc) = env.observation_space.shape\n",
    "        newshape = (size, size, oldc)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255,\n",
    "            shape=newshape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, frame):\n",
    "        height, width, _ = self.observation_space.shape\n",
    "        frame = cv2.resize(frame, (width, height), interpolation=cv2.INTER_AREA)\n",
    "        if frame.ndim == 2:\n",
    "            frame = frame[:,:,None]\n",
    "        return frame\n",
    "\n",
    "def create_mario_env(world, stage, version, use_coin_collector_env):\n",
    "    env = create_gym_env_from_level(world, stage, version, use_coin_collector_env)\n",
    "    env = JoypadSpace(env, COMPLEX_MOVEMENT)\n",
    "    env = SkipFrame(env, skip=4)\n",
    "    env = GrayScaleObservation(env, keep_dim=True)\n",
    "    env = ResizeEnv(env, size=84)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    env = VecFrameStack(env, 4, channels_order='last')\n",
    "    return env\n",
    "\n",
    "class MarioNet(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: gym.spaces.Box, features_dim):\n",
    "        super(MarioNet, self).__init__(observation_space, features_dim)\n",
    "        n_input_channels = observation_space.shape[0]\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        # Compute shape by doing one forward pass\n",
    "        with th.no_grad():\n",
    "            n_flatten = self.cnn(th.as_tensor(observation_space.sample()[None]).float()).shape[1]\n",
    "\n",
    "        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
    "\n",
    "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
    "        return self.linear(self.cnn(observations))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 84, 84, 4)\n"
     ]
    }
   ],
   "source": [
    "# <world> is a number in {1, 2, 3, 4, 5, 6, 7, 8} indicating the world\n",
    "world = 1\n",
    "# <stage> is a number in {1, 2, 3, 4} indicating the stage within a world\n",
    "stage = 1\n",
    "version = 3\n",
    "use_coin_collector_env = True\n",
    "\n",
    "env = create_mario_env(world, stage, version, use_coin_collector_env)\n",
    "\n",
    "env.reset()\n",
    "state, reward, done, info = env.step([0])\n",
    "print('state:', state.shape) #Color scale, height, width, num of stacks\n",
    "\n",
    "\n",
    "env = create_mario_env(world, stage, version, use_coin_collector_env)\n",
    "plays = 4"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_epoch = 1418000 #change as per the values inferred from the graph\n",
    "# best_model_path = 'mario/cc_model/best_model_{}.zip'.format(best_epoch)\n",
    "\n",
    "# # Load the best model\n",
    "# model = PPO.load(best_model_path)\n",
    "# record_env = RecordVideo(env, f\"mario_plays/cc_model/{best_epoch}\", name_prefix=\"mario_ppo\")\n",
    "\n",
    "# for ep in range(plays):\n",
    "#     state = record_env.reset()\n",
    "#     done = False\n",
    "#     while not done:\n",
    "#         action, _ = model.predict(state)\n",
    "#         state, reward, done, info = record_env.step(action)\n",
    "#         record_env.render()\n",
    "# print(f\"Episode {ep} done\")"
=======
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How far it gone to the right\n",
    "#\n",
    "#\n",
    "# Between time step t1 and t2 points from coin divide by 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ObjCInstance b'NSTrackingArea' has no attribute b'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m         action, _ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(state)\n\u001b[1;32m     13\u001b[0m         state, reward, done, info \u001b[38;5;241m=\u001b[39m record_env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m---> 14\u001b[0m         \u001b[43mrecord_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m done\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/mario_proj/lib/python3.8/site-packages/gym/core.py:295\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mario_proj/lib/python3.8/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:281\u001b[0m, in \u001b[0;36mVecEnvWrapper.render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[np\u001b[38;5;241m.\u001b[39mndarray]:\n\u001b[0;32m--> 281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mario_proj/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:87\u001b[0m, in \u001b[0;36mDummyVecEnv.render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;03mGym environment rendering. If there are multiple environments then\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03mthey are tiled together in one image via ``BaseVecEnv.render()``.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m:param mode: The rendering type.\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrender(mode\u001b[38;5;241m=\u001b[39mmode)\n",
      "File \u001b[0;32m~/miniconda3/envs/mario_proj/lib/python3.8/site-packages/gym/core.py:295\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mario_proj/lib/python3.8/site-packages/gym/core.py:295\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping similar frames: Wrapper.render at line 295 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/mario_proj/lib/python3.8/site-packages/gym/core.py:295\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mario_proj/lib/python3.8/site-packages/nes_py/nes_env.py:386\u001b[0m, in \u001b[0;36mNESEnv.render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer \u001b[38;5;241m=\u001b[39m ImageViewer(\n\u001b[1;32m    381\u001b[0m             caption\u001b[38;5;241m=\u001b[39mcaption,\n\u001b[1;32m    382\u001b[0m             height\u001b[38;5;241m=\u001b[39mSCREEN_HEIGHT,\n\u001b[1;32m    383\u001b[0m             width\u001b[38;5;241m=\u001b[39mSCREEN_WIDTH,\n\u001b[1;32m    384\u001b[0m         )\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;66;03m# show the screen on the image viewer\u001b[39;00m\n\u001b[0;32m--> 386\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mviewer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscreen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen\n",
      "File \u001b[0;32m~/miniconda3/envs/mario_proj/lib/python3.8/site-packages/nes_py/_image_viewer.py:138\u001b[0m, in \u001b[0;36mImageViewer.show\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_window\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_window\u001b[38;5;241m.\u001b[39mswitch_to()\n\u001b[0;32m--> 138\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_window\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_events\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# create an image data object\u001b[39;00m\n\u001b[1;32m    140\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpyglet\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mImageData(\n\u001b[1;32m    141\u001b[0m     frame\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    142\u001b[0m     frame\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    145\u001b[0m     pitch\u001b[38;5;241m=\u001b[39mframe\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m\n\u001b[1;32m    146\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/mario_proj/lib/python3.8/site-packages/pyglet/window/cocoa/__init__.py:312\u001b[0m, in \u001b[0;36mCocoaWindow.dispatch_events\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    308\u001b[0m event \u001b[38;5;241m=\u001b[39m NSApp\u001b[38;5;241m.\u001b[39mnextEventMatchingMask_untilDate_inMode_dequeue_(\n\u001b[1;32m    309\u001b[0m     cocoapy\u001b[38;5;241m.\u001b[39mNSAnyEventMask, \u001b[38;5;28;01mNone\u001b[39;00m, cocoapy\u001b[38;5;241m.\u001b[39mNSEventTrackingRunLoopMode, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event:\n\u001b[0;32m--> 312\u001b[0m     event_type \u001b[38;5;241m=\u001b[39m \u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m()\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;66;03m# Pass on all events.\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     NSApp\u001b[38;5;241m.\u001b[39msendEvent_(event)\n",
      "File \u001b[0;32m~/miniconda3/envs/mario_proj/lib/python3.8/site-packages/pyglet/libs/darwin/cocoapy/runtime.py:1009\u001b[0m, in \u001b[0;36mObjCInstance.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1007\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ObjCBoundMethod(method, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjc_class\u001b[38;5;241m.\u001b[39mptr)\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;66;03m# Otherwise raise an exception.\u001b[39;00m\n\u001b[0;32m-> 1009\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObjCInstance \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjc_class\u001b[38;5;241m.\u001b[39mname, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: ObjCInstance b'NSTrackingArea' has no attribute b'type'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "best_epoch = 1418000 #change as per the values inferred from the graph\n",
    "best_model_path = 'mario/cc_model/best_model_{}.zip'.format(best_epoch)\n",
    "\n",
    "# Load the best model\n",
    "model = PPO.load(best_model_path)\n",
    "record_env = RecordVideo(env, f\"mario_plays/cc_model/{best_epoch}\", name_prefix=\"mario_ppo\")\n",
    "\n",
    "for ep in range(plays):\n",
    "    state = record_env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _ = model.predict(state)\n",
    "        state, reward, done, info = record_env.step(action)\n",
    "        record_env.render()\n",
    "print(f\"Episode {ep} done\")"
>>>>>>> 0fb07151e8844acdf5e772f72a1a19fe86698bf9
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info[0][\"flag_get\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 84, 84, 4)\n"
     ]
    }
   ],
   "source": [
    "# <world> is a number in {1, 2, 3, 4, 5, 6, 7, 8} indicating the world\n",
    "world = 1\n",
    "# <stage> is a number in {1, 2, 3, 4} indicating the stage within a world\n",
    "stage = 1\n",
    "version = 3\n",
    "use_coin_collector_env = False\n",
    "\n",
    "env = create_mario_env(world, stage, version, use_coin_collector_env)\n",
    "\n",
    "env.reset()\n",
    "state, reward, done, info = env.step([0])\n",
    "print('state:', state.shape) #Color scale, height, width, num of stacks\n",
    "\n",
    "\n",
    "env = create_mario_env(world, stage, version, use_coin_collector_env)\n",
    "plays = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 0 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 1 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 2 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n",
      "Episode 3 done\n"
     ]
    }
   ],
   "source": [
    "best_epoch = 848000 #change as per the values inferred from the graph\n",
    "best_model_path = 'mario/reg_model/best_model_{}.zip'.format(best_epoch)\n",
    "\n",
    "# Load the best model\n",
    "model = PPO.load(best_model_path)\n",
    "record_env = RecordVideo(env, f\"mario_plays/reg_model/{best_epoch}\", name_prefix=\"mario_ppo\")\n",
    "\n",
    "for ep in range(plays):\n",
    "    state = record_env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _ = model.predict(state)\n",
    "        state, reward, done, info = record_env.step(action)\n",
    "        record_env.render()\n",
    "        print(f\"Episode {ep} done\")"
   ]
<<<<<<< HEAD
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best model record plays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cc_model_timestep = 2328000\n",
    "best_reg_model_timestep = 2273000\n",
    "\n",
    "best_cc_model = 'mario/cc_model/best_model_{}.zip'.format(best_cc_model_timestep)\n",
    "best_cc_model = PPO.load(best_cc_model)\n",
    "\n",
    "best_reg_model = 'mario/reg_model/best_model_{}.zip'.format(best_reg_model_timestep)\n",
    "best_reg_model = PPO.load(best_reg_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pass_rate(model=None, plays=1000, world=1, stage=1, version=3, use_coin_collector_env=True):\n",
    "    get_count = 0\n",
    "    coin_count = 0\n",
    "    coin_collection = []\n",
    "\n",
    "    env = env_setup(world=world, stage=stage, version=version, use_coin_collector_env=use_coin_collector_env)\n",
    "\n",
    "    for _ in tqdm(range(plays)):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = model.predict(state)\n",
    "            state, reward, done, info = env.step(action)\n",
    "        if info[0]['flag_get'] == True:\n",
    "            get_count += 1\n",
    "        coin_count_current_play = info[0]['coins']\n",
    "        coin_collection.append(coin_count_current_play)\n",
    "        coin_count += info[0]['coins']\n",
    "    # print(f'flag get count: {get_count}')\n",
    "    # print(f'total coin count: {coin_count}')\n",
    "\n",
    "    return get_count, coin_count, coin_collection\n",
    "\n",
    "def env_setup(world=1, stage=1, version=3, use_coin_collector_env=True):\n",
    "\n",
    "    env = create_mario_env(world=world, stage=stage, version=version, use_coin_collector_env=use_coin_collector_env)\n",
    "    env.reset()\n",
    "    state, reward, done, info = env.step([0])\n",
    "    print('state:', state.shape) #Color scale, height, width, num of stacks\n",
    "\n",
    "    env = create_mario_env(world=world, stage=stage, version=version, use_coin_collector_env=use_coin_collector_env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "371 996\n"
     ]
    }
   ],
   "source": [
    "# cc model trial 1\n",
    "pass_count_cc, coin_count_cc, coin_collected_t1_cc = pass_rate(best_cc_model, use_coin_collector_env=True)\n",
    "print(pass_count_cc, coin_count_cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 84, 84, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1000/1000 [44:44<00:00,  2.68s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "356 1036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# cc model trial 2\n",
    "pass_count_cc, coin_count_cc, coin_collected_t2_cc = pass_rate(best_cc_model, use_coin_collector_env=True)\n",
    "print(pass_count_cc, coin_count_cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 84, 84, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1000/1000 [44:23<00:00,  2.66s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "343 1005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# cc model trial 3\n",
    "pass_count_cc, coin_count_cc, coin_collected_t3_cc = pass_rate(best_cc_model, use_coin_collector_env=True)\n",
    "print(pass_count_cc, coin_count_cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "374 700\n"
     ]
    }
   ],
   "source": [
    "# reg model trial 1\n",
    "pass_count_reg, coin_count_reg = pass_rate(best_reg_model, use_coin_collector_env=False)\n",
    "print(pass_count_reg, coin_count_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 84, 84, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1000/1000 [48:48<00:00,  2.93s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382 667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# reg model trial 2\n",
    "pass_count_reg, coin_count_reg, coin_collected_t2_reg = pass_rate(best_reg_model, use_coin_collector_env=False)\n",
    "print(pass_count_reg, coin_count_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (1, 84, 84, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1000/1000 [47:59<00:00,  2.88s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "369 622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# reg model trial 3\n",
    "pass_count_reg, coin_count_reg, coin_collected_t3_cc = pass_rate(best_reg_model, use_coin_collector_env=False)\n",
    "print(pass_count_reg, coin_count_reg)"
   ]
=======
>>>>>>> 0fb07151e8844acdf5e772f72a1a19fe86698bf9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mario_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
