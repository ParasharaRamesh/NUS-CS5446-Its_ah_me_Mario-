{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Most of this notebook is inspired from -> https://www.kaggle.com/code/deeplyai/super-mario-bros-with-stable-baseline3-ppo"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Getting the colab environment ready\n",
    "\n",
    "Run the cell below only if running on colab"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For ensuring gym 0.21.0 works\n",
    "!pip install setuptools==65.5.0 pip==21\n",
    "!pip install wheel==0.38.0\n",
    "\n",
    "# for ensuring tensorboard works\n",
    "# !pip install tensorflow\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# regular rl stuff\n",
    "!pip -qq install stable-baselines3==1.6.0\n",
    "!pip install -qq gym-super-mario-bros"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Ensure that this path exists in google drive\n",
    "tensorboard_logdir = os.path.abspath(\"/content/drive/MyDrive/mario/model\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Tensorboard"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Importing packages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#standard packages\n",
    "import gym\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch as th\n",
    "from torch import nn\n",
    "import os\n",
    "\n",
    "# mario packages\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros import SuperMarioBrosEnv\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros.actions import *\n",
    "\n",
    "# For custom level\n",
    "from game.environment import create_gym_env_from_level\n",
    "\n",
    "# Import Frame Stacker Wrapper and GrayScaling Wrapper\n",
    "from gym.wrappers import GrayScaleObservation\n",
    "\n",
    "# Import Vectorization Wrappers\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv\n",
    "\n",
    "# Import algo\n",
    "from stable_baselines3 import A2C\n",
    "\n",
    "# Import Base Callback for saving models\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.vec_env import VecVideoRecorder, SubprocVecEnv, DummyVecEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing & Defining Environment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Custom rewards for keeping track of delta of coins collected and for registering this as a custom gym environment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CoinCollectorSuperMarioBrosEnv(SuperMarioBrosEnv):\n",
    "    #score btn 2 time frames can maybe go upto 8000 so we can just divide by 100 (reference https://www.mariowiki.com/Point)\n",
    "    reward_range = (-15, 100)\n",
    "\n",
    "    def __init__(self, rom_mode='vanilla', lost_levels=False, target=None):\n",
    "        super().__init__(rom_mode=rom_mode, lost_levels=lost_levels, target=target)\n",
    "\n",
    "        # variable to keep track of score deltas\n",
    "        self._score_last = 0\n",
    "\n",
    "    @property\n",
    "    def _score_reward(self):\n",
    "        _reward = self._score - self._score_last\n",
    "        self._score_last = self._score\n",
    "        return _reward/100\n",
    "\n",
    "    # This should override the parent function\n",
    "    def _get_reward(self):\n",
    "        return self._x_reward + self._score_reward + self._time_penalty + self._death_penalty\n",
    "\n",
    "'''\n",
    "The code below registers this new environment in gym for us to reference later. Code borrowed from _registration.py of gym_super_mario_bros\n",
    "'''\n",
    "def _register_coin_collector_mario_stage_env(id, **kwargs):\n",
    "    \"\"\"\n",
    "    Register a Super Mario Bros. (1/2) stage environment with OpenAI Gym.\n",
    "\n",
    "    Args:\n",
    "        id (str): id for the env to register\n",
    "        kwargs (dict): keyword arguments for the SuperMarioBrosEnv initializer\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    \"\"\"\n",
    "    # register the environment\n",
    "    gym.envs.registration.register(\n",
    "        id=id,\n",
    "        # entry_point='.:CoinCollectorSuperMarioBrosEnv',\n",
    "        entry_point=CoinCollectorSuperMarioBrosEnv,\n",
    "        max_episode_steps=9999999,\n",
    "        reward_threshold=9999999,\n",
    "        kwargs=kwargs,\n",
    "        nondeterministic=True,\n",
    "    )\n",
    "\n",
    "def _register_all_coin_collector_envs():\n",
    "    # a template for making individual stage environments\n",
    "    _ID_TEMPLATE = 'CoinCollectorSuperMarioBrosEnv-{}-{}-v{}'\n",
    "    # A list of ROM modes for each level environment\n",
    "    _ROM_MODES = [\n",
    "        'vanilla',\n",
    "        'downsample',\n",
    "        'pixel',\n",
    "        'rectangle'\n",
    "    ]\n",
    "\n",
    "    # iterate over all the rom modes, worlds (1-8), and stages (1-4)\n",
    "    for version, rom_mode in enumerate(_ROM_MODES):\n",
    "        for world in range(1, 9):\n",
    "            for stage in range(1, 5):\n",
    "                # create the target\n",
    "                target = (world, stage)\n",
    "                # setup the frame-skipping environment\n",
    "                env_id = _ID_TEMPLATE.format(world, stage, version)\n",
    "                print(f\"Registering Coin Collector {env_id} in gym for use later on.\")\n",
    "                _register_coin_collector_mario_stage_env(env_id, rom_mode=rom_mode, target=target)\n",
    "                print(f\"Successfully registered coin collector env {env_id}!\")\n",
    "\n",
    "def create_gym_env_from_level(world, stage, version, use_coin_collector_env):\n",
    "    level_suffix = f\"{world}-{stage}-v{version}\"\n",
    "    if not use_coin_collector_env:\n",
    "        level = f\"SuperMarioBros-{level_suffix}\"\n",
    "        env = gym_super_mario_bros.make(level)\n",
    "    else:\n",
    "        env_set = set(gym.envs.registration.registry.env_specs.copy().keys())\n",
    "        level = f\"CoinCollectorSuperMarioBrosEnv-{level_suffix}\"\n",
    "        if level not in env_set:\n",
    "            # register all these custom environments for the first time\n",
    "            _register_all_coin_collector_envs()\n",
    "\n",
    "        assert level in set(\n",
    "            gym.envs.registration.registry.env_specs.copy().keys()\n",
    "        ), f\"Looks like {level} was not registered correctly!\"\n",
    "        env = gym.make(level)\n",
    "\n",
    "    return env\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "By default, in each frame the game performs an action (a movement) and returns the reward for that action. What happens, is that to train the AI it is not necessary to make a move in each frame. That is why, the function executes the movement every X frames giving less work to do the training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, info"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "By default the enviroment is given by 240*256 pixels. In order to optimize our model it is not necessary to have so many pixels and that is why we can rescale our enviroment to a smaller scale."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ResizeEnv(gym.ObservationWrapper):\n",
    "    def __init__(self, env, size):\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        (oldh, oldw, oldc) = env.observation_space.shape\n",
    "        newshape = (size, size, oldc)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255,\n",
    "            shape=newshape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, frame):\n",
    "        height, width, _ = self.observation_space.shape\n",
    "        frame = cv2.resize(frame, (width, height), interpolation=cv2.INTER_AREA)\n",
    "        if frame.ndim == 2:\n",
    "            frame = frame[:,:,None]\n",
    "        return frame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Importing our custom environment function and including all of the processing we define the following function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_mario_env(world, stage, version, use_coin_collector_env):\n",
    "    env = create_gym_env_from_level(world, stage, version, use_coin_collector_env)\n",
    "    env = JoypadSpace(env, COMPLEX_MOVEMENT)\n",
    "    env = SkipFrame(env, skip=4)\n",
    "    env = GrayScaleObservation(env, keep_dim=True)\n",
    "    env = ResizeEnv(env, size=84)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    env = VecFrameStack(env, 4, channels_order='last')\n",
    "    return env\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating the environment\n",
    "\n",
    "NOTE: Change the use_coin_collector_env boolean to use the normal base env"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# <world> is a number in {1, 2, 3, 4, 5, 6, 7, 8} indicating the world\n",
    "world = 1\n",
    "# <stage> is a number in {1, 2, 3, 4} indicating the stage within a world\n",
    "stage = 1\n",
    "version = 3\n",
    "use_coin_collector_env = True\n",
    "\n",
    "env = create_mario_env(world, stage, version, use_coin_collector_env)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env.reset()\n",
    "state, reward, done, info = env.step([0])\n",
    "print('state:', state.shape) #Color scale, height, width, num of stacks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model definition"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MarioNet(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: gym.spaces.Box, features_dim):\n",
    "        super(MarioNet, self).__init__(observation_space, features_dim)\n",
    "        n_input_channels = observation_space.shape[0]\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        # Compute shape by doing one forward pass\n",
    "        with th.no_grad():\n",
    "            n_flatten = self.cnn(th.as_tensor(observation_space.sample()[None]).float()).shape[1]\n",
    "\n",
    "        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
    "\n",
    "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
    "        return self.linear(self.cnn(observations))\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=MarioNet,\n",
    "    features_extractor_kwargs=dict(features_dim=512),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next step consists of the creation of a file where the AI will save the results obtained in each iteration. In this way, later we will be able to visualize graphically the learning of our model.\n",
    "\n",
    "In this case, the average score, the average starting time and the best score obtained will be saved for each iteration."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reward_log_path = os.path.join(tensorboard_logdir, 'reward_log.csv')\n",
    "\n",
    "with open(reward_log_path, 'a') as f:\n",
    "    print('timesteps,reward,best_reward', file=f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This callback function will be in charge of writing the aforementioned data to the file. This function will be executed automatically each time an iteration has been completed."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(tensorboard_logdir, f'best_model_{self.n_calls}')\n",
    "            self.model.save(model_path)\n",
    "\n",
    "            total_reward = [0] * EPISODE_NUMBERS\n",
    "            total_time = [0] * EPISODE_NUMBERS\n",
    "            best_reward = 0\n",
    "\n",
    "            for i in range(EPISODE_NUMBERS):\n",
    "                state = env.reset()  # reset for each new trial\n",
    "                done = False\n",
    "                total_reward[i] = 0\n",
    "                total_time[i] = 0\n",
    "                while not done and total_time[i] < MAX_TIMESTEP_TEST:\n",
    "                    action, _ = model.predict(state)\n",
    "                    state, reward, done, info = env.step(action)\n",
    "\n",
    "                    # This should render it\n",
    "                    # env.render()\n",
    "\n",
    "                    total_reward[i] += reward[0]\n",
    "                    total_time[i] += 1\n",
    "\n",
    "                if total_reward[i] > best_reward:\n",
    "                    best_reward = total_reward[i]\n",
    "                    best_epoch = self.n_calls\n",
    "\n",
    "                state = env.reset()  # reset for each new trial\n",
    "\n",
    "            print('time steps:', self.n_calls, '/', TOTAL_TIMESTEP_NUMB)\n",
    "            print('average reward:', (sum(total_reward) / EPISODE_NUMBERS),\n",
    "                  'average time:', (sum(total_time) / EPISODE_NUMBERS),\n",
    "                  'best_reward:', best_reward)\n",
    "\n",
    "            with open(reward_log_path, 'a') as f:\n",
    "                print(self.n_calls, ',', sum(total_reward) / EPISODE_NUMBERS, ',', best_reward, file=f)\n",
    "\n",
    "        return True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Model Param\n",
    "# CHECK_FREQ_NUMB = 10000\n",
    "CHECK_FREQ_NUMB = 1000\n",
    "TOTAL_TIMESTEP_NUMB = 5000000\n",
    "LEARNING_RATE = 0.0001\n",
    "GAE = 1.0\n",
    "ENT_COEF = 0.01\n",
    "N_STEPS = 512\n",
    "GAMMA = 0.9\n",
    "BATCH_SIZE = 64\n",
    "N_EPOCHS = 10\n",
    "\n",
    "# Test Param\n",
    "EPISODE_NUMBERS = 20\n",
    "MAX_TIMESTEP_TEST = 1000"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=CHECK_FREQ_NUMB, save_path=tensorboard_logdir)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Training\n",
    "\n",
    "Can change the algorithm here to other popular RL algorithms imported from stable baselines, no other changes needed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = A2C('CnnPolicy', env, verbose=2, policy_kwargs=policy_kwargs, tensorboard_log=tensorboard_logdir,\n",
    "            learning_rate=LEARNING_RATE, n_steps=N_STEPS, gamma=GAMMA, gae_lambda=GAE,\n",
    "            ent_coef=ENT_COEF)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=TOTAL_TIMESTEP_NUMB, callback=callback, log_interval=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tensorboard"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%tensorboard --logdir $tensorboard_logdir"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model testing\n",
    "\n",
    "Get the best epoch we would need to check the graph and see the best epoch from there.\n",
    "\n",
    "Run the below cells only locally!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_epoch = 5000 #change as per the values inferred from"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_model_path = os.path.join(tensorboard_logdir, 'best_model_{}'.format(best_epoch))\n",
    "model = A2C.load(best_model_path)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "done = True\n",
    "plays = 0;\n",
    "wins = 0;\n",
    "while plays < 100:\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        if info[0][\"flag_get\"]:\n",
    "          wins += 1\n",
    "        plays += 1\n",
    "    action, _ = model.predict(state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "print(\"Model win rate: \" + str(wins) + \"%\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
